test.sites
i=9
Noble::tis.pq.test(site = test.sites[i], dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)
parsed.results=Noble::parse.results(test.dir=testFullDir, write.summary=T)
View(parsed.results)
for(i in 1:length(test.sites)){
sink<-try(
Noble::tis.pq.test(site = test.sites[i], dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)
)
}
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
library(Noble)
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
bgn_temp <- as.Date(paste0(bgn.month, "-01"), tz="UTC")
end_temp <- as.Date(paste0(end.month, "-01"), tz="UTC")
date_range<-substr(seq.Date(bgn_temp, end_temp, "month"), 0, 7)
site_meta=jsonlite::read_json(paste0("http://data.neonscience.org/api/v0/sites/", site))$data
prod_meta=jsonlite::read_json(paste0("http://data.neonscience.org/api/v0/products/", dpID))$data
prod_indx=grep(site_meta$dataProducts, pattern = dpID)
site_indx=grep(prod_meta$siteCodes, pattern = site)
if(length(prod_indx)==0){
stop(paste0(dpID, " is not currently available at ", site, " via the API."))
}
site_options=data.frame(avail_months=unlist(site_meta$dataProducts[[prod_indx]]$availableMonths), urls= unlist(site_meta$dataProducts[[prod_indx]]$availableDataUrls))
# Stop if no data
if(length(site_options$avail_months)==0){stop(paste0(dpID, " is missing at ", site))}
all_data_urls <- unlist(unique(site_options$urls))
#construct temporary API call urls
url_index<-lapply(date_range, function(x) grep(pattern=x, all_data_urls))
temp_data_urls<-all_data_urls[unlist(url_index)]
if(length(temp_data_urls)==0){stop("Data was missing in specified date range at ", site, ". Check ", dpID, " avalability with NEON.avail")}
#For found DPs, given the Kpi, pull hosted metadata via API
api_data<-(lapply(temp_data_urls, function(x) jsonlite::read_json(path = as.character(x))))
# build a list of URLs served by the API
url_list<-c()
i<-1
for(i in 1:length(api_data)){
tempList<-api_data[[i]]$data$files
listLeng<-length(tempList)
if(listLeng==0){break()}
for(j in 1:listLeng){
url_list<-append(url_list, tempList[[j]]$url)
}
}
# Weed out XML links
url_list=url_list[!(grepl(pattern = "xml", x= url_list))]
#Try to handle name excpetions
exceptions=c("DP1.00005.001", "DP1.00041.001")
if((dpID %in% exceptions)){ #Why, oh why does bio temp have to be different on the API
url_list<-url_list[grepl(pattern = paste0(time.agr, "_min*"), x= url_list)]
}else{
url_list=url_list[grepl(pattern = paste0(time.agr,"min*"), x= url_list)|grepl(pattern = paste0(time.agr, "_min*"), x= url_list)] #should catch unknown exceptions
}
#Looking for location info
loc_list_temp=stringr::str_extract(string=url_list, pattern = paste0(dpID, "\\.\\d\\d\\d\\.\\d\\d\\d\\.\\d\\d\\d"))
loc_list=stringr::str_sub(loc_list_temp, start = 15, end = 21)
if(all(is.na(loc_list_temp))){
loc_list_temp=stringr::str_extract(string=url_list, pattern ="\\.\\d\\d\\d\\.\\d\\d\\d\\.\\d\\d\\d")
loc_list=stringr::str_sub(loc_list_temp, start = 1, end = 8)
}
dp_list<-rep(dpID, times=length(loc_list))
call.df<-as.data.frame(cbind(url_list, dp_list, loc_list))
# Order the call.df by data product, then location
call.df<-call.df[order(call.df$dp_list, call.df$url_list),]
call.df=call.df[which(grepl(x=call.df$url_list, pattern=package)),] #Keep only our package type
call.df=call.df[which(grepl(x=call.df$url_list, pattern="\\.csv")),] #Keep only CSVs
call.df=call.df[which(!grepl(x=call.df$url_list, pattern="variables")),] #weed out varaible tables
return(call.df)
class(call.df)
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
bgn_temp <- as.Date(paste0(bgn.month, "-01"), tz="UTC")
end_temp <- as.Date(paste0(end.month, "-01"), tz="UTC")
date_range<-substr(seq.Date(bgn_temp, end_temp, "month"), 0, 7)
site_meta=jsonlite::read_json(paste0("http://data.neonscience.org/api/v0/sites/", site))$data
prod_meta=jsonlite::read_json(paste0("http://data.neonscience.org/api/v0/products/", dpID))$data
prod_indx=grep(site_meta$dataProducts, pattern = dpID)
site_indx=grep(prod_meta$siteCodes, pattern = site)
if(length(prod_indx)==0){
stop(paste0(dpID, " is not currently available at ", site, " via the API."))
}
site_options=data.frame(avail_months=unlist(site_meta$dataProducts[[prod_indx]]$availableMonths), urls= unlist(site_meta$dataProducts[[prod_indx]]$availableDataUrls))
#####
# Stop if no data
if(length(site_options$avail_months)==0){stop(paste0(dpID, " is missing at ", site))}
all_data_urls <- unlist(unique(site_options$urls))
#construct temporary API call urls
url_index<-lapply(date_range, function(x) grep(pattern=x, all_data_urls))
temp_data_urls<-all_data_urls[unlist(url_index)]
if(length(temp_data_urls)==0){stop("Data was missing in specified date range at ", site, ". Check ", dpID, " avalability with NEON.avail")}
#For found DPs, given the Kpi, pull hosted metadata via API
api_data<-(lapply(temp_data_urls, function(x) jsonlite::read_json(path = as.character(x))))
test <- "Soil Heat Flux Process Quality"
testSubDir <- "TisSoilHeatFluxProcessQuality"
if(grepl("darwin", version$os))
{
mountPoint<-"/Volumes/neon/" #Mac
}else{
mountPoint<-"N:/" #Windows
}
dirCommBase = paste0(mountPoint, "Science/Science Commissioning Archive/SiteAndPayload/")
# should not need to edit these...
myTitle <- paste("TIS Commissioning Test Report:", test, "Assessment")
rmdFile <- paste( testSubDir, ".Rmd", sep="")
testFullDir <- paste(dirCommBase, testSubDir, "/", sep="")
dir = testFullDir
dpID<-"DP1.00040.001"
prin.vars<-c("SHF")
bgn.month<-"2018-02"
end.month<-"2018-03"
package="basic"
time.agr = 30
#tis_test_sites=Noble::test.sites(dpID = dpID, bgn.month = bgn.month, end.month = end.month)
tis_test_sites=c("HARV", "BART", "SERC", "SCBI", "BLAN", "OSBS", "JERC", "DSNY", "LAJA", "GUAN", "UNDE", "UKFS", "KONZ", "KONA", "ORNL", "MLBS", "GRSM", "TALL", "LENO", "DELA", "WOOD", "TREE", "NOGP", "DCFS", "STER", "RMNP", "CPER", "OAES", "CLBJ", "NIWO", "MOAB",  "SRER", "JORN", "ONAQ", "ABBY", "SJER", "BARR", "TOOL", "DEJU", "HEAL")
if(file.exists(paste0(dir, "/Common/summary_results.csv"))){
summary_results=read.csv(paste0(dir, "/Common/summary_results.csv"))
passed.sites=summary_results$site[as.logical(summary_results$passed)]
}else{passed.sites==c()}
#passed.sites=c("HARV", "BART",  "TREE", "KONA", "TOOL", "DEJU", "SERC", "KONZ", "BLAN") #?"JERC",
test.sites=tis_test_sites[which(!tis_test_sites %in% passed.sites)] #filter out sites that passed testing
#test.sites=test.sites[-which(test.sites %in% bad.sites)]
#test.sites="HEAL"
if(!dir.exists(dir)){
dir.create(dir)
}
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
site=test.sites[i]
site=test.sites[3]
bgn_temp <- as.Date(paste0(bgn.month, "-01"), tz="UTC")
end_temp <- as.Date(paste0(end.month, "-01"), tz="UTC")
date_range<-substr(seq.Date(bgn_temp, end_temp, "month"), 0, 7)
site_meta=jsonlite::read_json(paste0("http://data.neonscience.org/api/v0/sites/", site))$data
prod_meta=jsonlite::read_json(paste0("http://data.neonscience.org/api/v0/products/", dpID))$data
prod_indx=grep(site_meta$dataProducts, pattern = dpID)
site_indx=grep(prod_meta$siteCodes, pattern = site)
if(length(prod_indx)==0){
stop(paste0(dpID, " is not currently available at ", site, " via the API."))
}
site_options=data.frame(avail_months=unlist(site_meta$dataProducts[[prod_indx]]$availableMonths), urls= unlist(site_meta$dataProducts[[prod_indx]]$availableDataUrls))
#####
# Stop if no data
if(length(site_options$avail_months)==0){stop(paste0(dpID, " is missing at ", site))}
all_data_urls <- unlist(unique(site_options$urls))
#construct temporary API call urls
url_index<-lapply(date_range, function(x) grep(pattern=x, all_data_urls))
temp_data_urls<-all_data_urls[unlist(url_index)]
if(length(temp_data_urls)==0){stop("Data was missing in specified date range at ", site, ". Check ", dpID, " avalability with NEON.avail")}
#For found DPs, given the Kpi, pull hosted metadata via API
api_data<-(lapply(temp_data_urls, function(x) jsonlite::read_json(path = as.character(x))))
View(api_data)
#For found DPs, given the Kpi, pull hosted metadata via API
api_data<-(lapply(temp_data_urls, function(x) jsonlite::read_json(path = as.character(x), simplifyVector = T)))
library(Noble)
version(jsonlite)
version
library(Noble)
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars))
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
lapply(test.sites, function(x) Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars))
lapply(test.sites, try(function(x) Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
test.sites
?jsonlite::fromJSON
library(Noble)
bgn_temp <- as.Date(paste0(bgn.month, "-01"), tz="UTC")
end_temp <- as.Date(paste0(end.month, "-01"), tz="UTC")
date_range<-substr(seq.Date(bgn_temp, end_temp, "month"), 0, 7)
site_meta=jsonlite::read_json(paste0("http://data.neonscience.org/api/v0/sites/", site))$data
prod_meta=jsonlite::read_json(paste0("http://data.neonscience.org/api/v0/products/", dpID))$data
prod_indx=grep(site_meta$dataProducts, pattern = dpID)
site_indx=grep(prod_meta$siteCodes, pattern = site)
if(length(prod_indx)==0){
stop(paste0(dpID, " is not currently available at ", site, " via the API."))
}
site_options=data.frame(avail_months=unlist(site_meta$dataProducts[[prod_indx]]$availableMonths), urls= unlist(site_meta$dataProducts[[prod_indx]]$availableDataUrls))
# Stop if no data
if(length(site_options$avail_months)==0){stop(paste0(dpID, " is missing at ", site))}
all_data_urls <- unlist(unique(site_options$urls))
#construct temporary API call urls
url_index<-lapply(date_range, function(x) grep(pattern=x, all_data_urls))
temp_data_urls<-all_data_urls[unlist(url_index)]
if(length(temp_data_urls)==0){stop("Data was missing in specified date range at ", site, ". Check ", dpID, " avalability with NEON.avail")}
#For found DPs, given the Kpi, pull hosted metadata via API
api_data<-(lapply(temp_data_urls, function(x) jsonlite::read_json(path = as.character(x), simplifyVector = F)))
# build a list of URLs served by the API
url_list<-c()
i<-1
for(i in 1:length(api_data)){
tempList<-api_data[[i]]$data$files
listLeng<-length(tempList)
if(listLeng==0){break()}
for(j in 1:listLeng){
url_list<-append(url_list, tempList[[j]]$url)
}
}
# Weed out XML links
url_list=url_list[!(grepl(pattern = "xml", x= url_list))]
#Try to handle name excpetions
exceptions=c("DP1.00005.001", "DP1.00041.001")
if((dpID %in% exceptions)){ #Why, oh why does bio temp have to be different on the API
url_list<-url_list[grepl(pattern = paste0(time.agr, "_min*"), x= url_list)]
}else{
url_list=url_list[grepl(pattern = paste0(time.agr,"min*"), x= url_list)|grepl(pattern = paste0(time.agr, "_min*"), x= url_list)] #should catch unknown exceptions
}
#Looking for location info
loc_list_temp=stringr::str_extract(string=url_list, pattern = paste0(dpID, "\\.\\d\\d\\d\\.\\d\\d\\d\\.\\d\\d\\d"))
loc_list=stringr::str_sub(loc_list_temp, start = 15, end = 21)
if(all(is.na(loc_list_temp))){
loc_list_temp=stringr::str_extract(string=url_list, pattern ="\\.\\d\\d\\d\\.\\d\\d\\d\\.\\d\\d\\d")
loc_list=stringr::str_sub(loc_list_temp, start = 1, end = 8)
}
dp_list<-rep(dpID, times=length(loc_list))
call.df<-as.data.frame(cbind(url_list, dp_list, loc_list))
# Order the call.df by data product, then location
call.df<-call.df[order(call.df$dp_list, call.df$url_list),]
call.df=call.df[which(grepl(x=call.df$url_list, pattern=package)),] #Keep only our package type
call.df=call.df[which(grepl(x=call.df$url_list, pattern="\\.csv")),] #Keep only CSVs
call.df=call.df[which(!grepl(x=call.df$url_list, pattern="variables")),] #weed out varaible tables
call.df
class(call.df$url_list)
class(call.df$dp_list)
class(call.df$loc_list)
#For found DPs, given the Kpi, pull hosted metadata via API
api_data<-(lapply(temp_data_urls, function(x) jsonlite::read_json(path = as.character(x), simplifyVector = F, flatten=F)))
library(Noble)
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
lapply(test.sites, function(x) try(Noble:::.gen.call.df(bgn.month=bgn.month,
end.month=end.month,
site=site, dpID=dpID,
time.agr=time.agr,
package=package)))
test=lapply(test.sites, function(x) try(Noble:::.gen.call.df(bgn.month=bgn.month,
end.month=end.month,
site=site, dpID=dpID,
time.agr=time.agr,
package=package)))
test
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
date_range
call.df$url_list[grepl(x=call.df$url_list, pattern = m)]
m=date_range[1]
call.df$url_list[grepl(x=call.df$url_list, pattern = m)]
data.wad=lapply(date_range, function(m) lapply(call.df$url_list[grepl(x=call.df$url_list, pattern = m)],
function(l) as.data.frame(read.csv(as.character(l)), stringsAsFactors = F))) #Get all data in one lump, (list of lists of data frames)
data.wad
data.lump=do.call(rbind, data.wad) #make into data frame of lists, with dimensions nrow=n_months, ncol=n_measurementLocations
data.chunk=lapply(seq(length(data.lump[1,])), function(x) do.call(rbind, data.lump[,x])) # merge down rows, so that only data frames of measurement levels exist
class(data.chunk)
## Clean up column naming (apply location info to measurement columns)
for(i in 1:length(data.chunk)){
colnames(data.chunk[[i]])[which(!grepl(x = colnames(data.chunk[[i]]), pattern = "time", ignore.case = T))]=
paste0(colnames(data.chunk[[i]][which(!grepl(x = names(data.chunk[[i]]), pattern = "time", ignore.case = T))]), ".", unique(call.df$loc_list)[i])
data.chunk[[i]]$startDateTime=as.POSIXct(data.chunk[[i]]$startDateTime, format="%Y-%m-%dT%H:%M:%SZ", tz="UTC")
}
library(Noble)
test <- "Soil Heat Flux Process Quality"
testSubDir <- "TisSoilHeatFluxProcessQuality"
if(grepl("darwin", version$os))
{
mountPoint<-"/Volumes/neon/" #Mac
}else{
mountPoint<-"N:/" #Windows
}
dirCommBase = paste0(mountPoint, "Science/Science Commissioning Archive/SiteAndPayload/")
# should not need to edit these...
myTitle <- paste("TIS Commissioning Test Report:", test, "Assessment")
rmdFile <- paste( testSubDir, ".Rmd", sep="")
testFullDir <- paste(dirCommBase, testSubDir, "/", sep="")
dir = testFullDir
dpID<-"DP1.00040.001"
prin.vars<-c("SHF")
bgn.month<-"2018-02"
end.month<-"2018-03"
package="basic"
time.agr = 30
#tis_test_sites=Noble::test.sites(dpID = dpID, bgn.month = bgn.month, end.month = end.month)
tis_test_sites=c("HARV", "BART", "SERC", "SCBI", "BLAN", "OSBS", "JERC", "DSNY", "LAJA", "GUAN", "UNDE", "UKFS", "KONZ", "KONA", "ORNL", "MLBS", "GRSM", "TALL", "LENO", "DELA", "WOOD", "TREE", "NOGP", "DCFS", "STER", "RMNP", "CPER", "OAES", "CLBJ", "NIWO", "MOAB",  "SRER", "JORN", "ONAQ", "ABBY", "SJER", "BARR", "TOOL", "DEJU", "HEAL")
if(file.exists(paste0(dir, "/Common/summary_results.csv"))){
summary_results=read.csv(paste0(dir, "/Common/summary_results.csv"))
passed.sites=summary_results$site[as.logical(summary_results$passed)]
}else{passed.sites==c()}
#passed.sites=c("HARV", "BART",  "TREE", "KONA", "TOOL", "DEJU", "SERC", "KONZ", "BLAN") #?"JERC",
test.sites=tis_test_sites[which(!tis_test_sites %in% passed.sites)] #filter out sites that passed testing
#test.sites=test.sites[-which(test.sites %in% bad.sites)]
#test.sites="HEAL"
if(!dir.exists(dir)){
dir.create(dir)
}
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
jsonlite:::fromJSON
jsonlite:::fromJSON_string
library(Noble)
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
library(Noble)
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
??jsonlite
library(Noble)
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
end_temp<-end_temp-lubridate::minutes(time.agr)-lubridate::seconds(1)
end_temp<-end_temp-lubridate::minutes(time.agr)-lubridate::seconds(1)
end_temp
# make a reference sequence
ref_seq<-Noble::help.time.seq(from=bgn_temp, to=end_temp+lubridate::seconds(1), time.agr = time.agr)
ref_seq
test <- "Soil Heat Flux Process Quality"
testSubDir <- "TisSoilHeatFluxProcessQuality"
if(grepl("darwin", version$os))
{
mountPoint<-"/Volumes/neon/" #Mac
}else{
mountPoint<-"N:/" #Windows
}
dirCommBase = paste0(mountPoint, "Science/Science Commissioning Archive/SiteAndPayload/")
# should not need to edit these...
myTitle <- paste("TIS Commissioning Test Report:", test, "Assessment")
rmdFile <- paste( testSubDir, ".Rmd", sep="")
testFullDir <- paste(dirCommBase, testSubDir, "/", sep="")
dir = testFullDir
dpID<-"DP1.00040.001"
prin.vars<-c("SHF")
bgn.month<-"2018-02"
end.month<-"2018-03"
package="basic"
time.agr = 30
#tis_test_sites=Noble::test.sites(dpID = dpID, bgn.month = bgn.month, end.month = end.month)
tis_test_sites=c("HARV", "BART", "SERC", "SCBI", "BLAN", "OSBS", "JERC", "DSNY", "LAJA", "GUAN", "UNDE", "UKFS", "KONZ", "KONA", "ORNL", "MLBS", "GRSM", "TALL", "LENO", "DELA", "WOOD", "TREE", "NOGP", "DCFS", "STER", "RMNP", "CPER", "OAES", "CLBJ", "NIWO", "MOAB",  "SRER", "JORN", "ONAQ", "ABBY", "SJER", "BARR", "TOOL", "DEJU", "HEAL")
if(file.exists(paste0(dir, "/Common/summary_results.csv"))){
summary_results=read.csv(paste0(dir, "/Common/summary_results.csv"))
passed.sites=summary_results$site[as.logical(summary_results$passed)]
}else{passed.sites==c()}
#passed.sites=c("HARV", "BART",  "TREE", "KONA", "TOOL", "DEJU", "SERC", "KONZ", "BLAN") #?"JERC",
test.sites=tis_test_sites[which(!tis_test_sites %in% passed.sites)] #filter out sites that passed testing
#test.sites=test.sites[-which(test.sites %in% bad.sites)]
#test.sites="HEAL"
if(!dir.exists(dir)){
dir.create(dir)
}
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
test=lapply(test.sites, function(x) try(Noble:::.gen.call.df(bgn.month=bgn.month,
end.month=end.month,
site=site, dpID=dpID,
time.agr=time.agr,
package=package)))
parsed.results=Noble::parse.results(test.dir=testFullDir, write.summary=T)
parsed.results=Noble::parse.results(test.dir=testFullDir, write.summary=T)
test <- "Soil Heat Flux Process Quality"
testSubDir <- "TisSoilHeatFluxProcessQuality"
if(grepl("darwin", version$os))
{
mountPoint<-"/Volumes/neon/" #Mac
}else{
mountPoint<-"N:/" #Windows
}
dirCommBase = paste0(mountPoint, "Science/Science Commissioning Archive/SiteAndPayload/")
# should not need to edit these...
myTitle <- paste("TIS Commissioning Test Report:", test, "Assessment")
rmdFile <- paste( testSubDir, ".Rmd", sep="")
testFullDir <- paste(dirCommBase, testSubDir, "/", sep="")
dir = testFullDir
dpID<-"DP1.00040.001"
prin.vars<-c("SHF")
bgn.month<-"2018-02"
end.month<-"2018-03"
package="basic"
time.agr = 30
#tis_test_sites=Noble::test.sites(dpID = dpID, bgn.month = bgn.month, end.month = end.month)
tis_test_sites=c("HARV", "BART", "SERC", "SCBI", "BLAN", "OSBS", "JERC", "DSNY", "LAJA", "GUAN", "UNDE", "UKFS", "KONZ", "KONA", "ORNL", "MLBS", "GRSM", "TALL", "LENO", "DELA", "WOOD", "TREE", "NOGP", "DCFS", "STER", "RMNP", "CPER", "OAES", "CLBJ", "NIWO", "MOAB",  "SRER", "JORN", "ONAQ", "ABBY", "SJER", "BARR", "TOOL", "DEJU", "HEAL")
if(file.exists(paste0(dir, "/Common/summary_results.csv"))){
summary_results=read.csv(paste0(dir, "/Common/summary_results.csv"))
passed.sites=summary_results$site[as.logical(summary_results$passed)]
}else{passed.sites==c()}
#passed.sites=c("HARV", "BART",  "TREE", "KONA", "TOOL", "DEJU", "SERC", "KONZ", "BLAN") #?"JERC",
test.sites=tis_test_sites[which(!tis_test_sites %in% passed.sites)] #filter out sites that passed testing
#test.sites=test.sites[-which(test.sites %in% bad.sites)]
#test.sites="HEAL"
if(!dir.exists(dir)){
dir.create(dir)
}
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
test=lapply(test.sites, function(x) try(Noble:::.gen.call.df(bgn.month=bgn.month,
end.month=end.month,
site=site, dpID=dpID,
time.agr=time.agr,
package=package)))
parsed.results=Noble::parse.results(test.dir=testFullDir, write.summary=T)
install.packages("lubridate")
install.packages("yaml", "lubridate")
install.packages("yaml", "lubridate")
install.packages("yaml", "lubridate")
install.packages("yaml", "lubridate")
library(lubridate)
install.packages("jsonlite", "stringi", "stringr", "ggplot2", "plotly", "reshape2", "plyr", "dplyr", "roxygen2")
install.packages(c("jsonlite", "stringi", "stringr", "ggplot2", "plotly", "reshape2", "plyr", "dplyr", "roxygen2"))
test <- "Soil Heat Flux Process Quality"
testSubDir <- "TisSoilHeatFluxProcessQuality"
if(grepl("darwin", version$os))
{
mountPoint<-"/Volumes/neon/" #Mac
}else{
mountPoint<-"N:/" #Windows
}
dirCommBase = paste0(mountPoint, "Science/Science Commissioning Archive/SiteAndPayload/")
# should not need to edit these...
myTitle <- paste("TIS Commissioning Test Report:", test, "Assessment")
rmdFile <- paste( testSubDir, ".Rmd", sep="")
testFullDir <- paste(dirCommBase, testSubDir, "/", sep="")
dir = testFullDir
dpID<-"DP1.00040.001"
prin.vars<-c("SHF")
bgn.month<-"2018-02"
end.month<-"2018-03"
package="basic"
time.agr = 30
#tis_test_sites=Noble::test.sites(dpID = dpID, bgn.month = bgn.month, end.month = end.month)
tis_test_sites=c("HARV", "BART", "SERC", "SCBI", "BLAN", "OSBS", "JERC", "DSNY", "LAJA", "GUAN", "UNDE", "UKFS", "KONZ", "KONA", "ORNL", "MLBS", "GRSM", "TALL", "LENO", "DELA", "WOOD", "TREE", "NOGP", "DCFS", "STER", "RMNP", "CPER", "OAES", "CLBJ", "NIWO", "MOAB",  "SRER", "JORN", "ONAQ", "ABBY", "SJER", "BARR", "TOOL", "DEJU", "HEAL")
if(file.exists(paste0(dir, "/Common/summary_results.csv"))){
summary_results=read.csv(paste0(dir, "/Common/summary_results.csv"))
passed.sites=summary_results$site[as.logical(summary_results$passed)]
}else{passed.sites==c()}
#passed.sites=c("HARV", "BART",  "TREE", "KONA", "TOOL", "DEJU", "SERC", "KONZ", "BLAN") #?"JERC",
test.sites=tis_test_sites[which(!tis_test_sites %in% passed.sites)] #filter out sites that passed testing
#test.sites=test.sites[-which(test.sites %in% bad.sites)]
#test.sites="HEAL"
if(!dir.exists(dir)){
dir.create(dir)
}
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
test=lapply(test.sites, function(x) try(Noble:::.gen.call.df(bgn.month=bgn.month,
end.month=end.month,
site=site, dpID=dpID,
time.agr=time.agr,
package=package)))
parsed.results=Noble::parse.results(test.dir=testFullDir, write.summary=T)
install.packages(c("Rcpp", "hellno", "RNRCS"))
test <- "Soil Heat Flux Process Quality"
testSubDir <- "TisSoilHeatFluxProcessQuality"
if(grepl("darwin", version$os))
{
mountPoint<-"/Volumes/neon/" #Mac
}else{
mountPoint<-"N:/" #Windows
}
dirCommBase = paste0(mountPoint, "Science/Science Commissioning Archive/SiteAndPayload/")
# should not need to edit these...
myTitle <- paste("TIS Commissioning Test Report:", test, "Assessment")
rmdFile <- paste( testSubDir, ".Rmd", sep="")
testFullDir <- paste(dirCommBase, testSubDir, "/", sep="")
dir = testFullDir
dpID<-"DP1.00040.001"
prin.vars<-c("SHF")
bgn.month<-"2018-02"
end.month<-"2018-03"
package="basic"
time.agr = 30
#tis_test_sites=Noble::test.sites(dpID = dpID, bgn.month = bgn.month, end.month = end.month)
tis_test_sites=c("HARV", "BART", "SERC", "SCBI", "BLAN", "OSBS", "JERC", "DSNY", "LAJA", "GUAN", "UNDE", "UKFS", "KONZ", "KONA", "ORNL", "MLBS", "GRSM", "TALL", "LENO", "DELA", "WOOD", "TREE", "NOGP", "DCFS", "STER", "RMNP", "CPER", "OAES", "CLBJ", "NIWO", "MOAB",  "SRER", "JORN", "ONAQ", "ABBY", "SJER", "BARR", "TOOL", "DEJU", "HEAL")
if(file.exists(paste0(dir, "/Common/summary_results.csv"))){
summary_results=read.csv(paste0(dir, "/Common/summary_results.csv"))
passed.sites=summary_results$site[as.logical(summary_results$passed)]
}else{passed.sites==c()}
#passed.sites=c("HARV", "BART",  "TREE", "KONA", "TOOL", "DEJU", "SERC", "KONZ", "BLAN") #?"JERC",
test.sites=tis_test_sites[which(!tis_test_sites %in% passed.sites)] #filter out sites that passed testing
#test.sites=test.sites[-which(test.sites %in% bad.sites)]
#test.sites="HEAL"
if(!dir.exists(dir)){
dir.create(dir)
}
lapply(test.sites, function(x) try(Noble::tis.pq.test(site = x, dpID = dpID, bgn.month = bgn.month, end.month = end.month,
time.agr = time.agr, package=package, save.dir=testFullDir, prin.vars=prin.vars)))
test=lapply(test.sites, function(x) try(Noble:::.gen.call.df(bgn.month=bgn.month,
end.month=end.month,
site=site, dpID=dpID,
time.agr=time.agr,
package=package)))
parsed.results=Noble::parse.results(test.dir=testFullDir, write.summary=T)
View(parsed.results)
pass=out[out$data_quantity>out$quant_threshold&out$data_validity>out$valid_threshold,]
out=parsed.results
pass=out[out$data_quantity>out$quant_threshold&out$data_validity>out$valid_threshold,]
View(pass)
install.packages("devtools")
install.packages("digest")
install.packages("roxygen2")
install.packages("roxygen2")
system.file(package="rvest")
system.file(package="RNRCS")
